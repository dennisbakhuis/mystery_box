{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a6cf9e-759d-441f-a786-8d72b8ebae58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mystery box - 3 - Train DONUT based distance detector\n",
    "### Dennis Bakhuis - 10th November 2022\n",
    "### https://linkedin.com/in/dennisbakhuis/\n",
    "\n",
    "Great Donut article by Phillipp Schmid: https://www.philschmid.de/fine-tuning-donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934af6f-6d08-4fe3-8fe8-94a55a893abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    DonutProcessor,\n",
    "    VisionEncoderDecoderModel, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from huggingface_hub import HfFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462619f-4c9c-4cad-9301-26765fd34c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bakhuisdennis/mystery_box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345acfe-25b5-4aeb-83d5-896c273edaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfe07a-cd7f-44e4-8830-87e9580bd97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541792a-9092-4948-b62f-b1964cc1af7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5249a75d-06a9-4f26-826a-4dc20970dfbd",
   "metadata": {},
   "source": [
    "## Change json to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf299a4-9dcd-4a0c-b261-b0f6a66113f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_special_tokens = []\n",
    "task_token_start = \"<s>\" \n",
    "task_token_end = \"</s>\" \n",
    "\n",
    "def json2token(\n",
    "    obj, \n",
    "    update_special_tokens_for_json_key: bool = True, \n",
    "    sort_json_key: bool = True,\n",
    "):\n",
    "    \"\"\"Function based on https://github.com/clovaai/donut/blob/master/donut/model.py#L497.\"\"\"\n",
    "    if type(obj) == dict:\n",
    "        if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "            return obj[\"text_sequence\"]\n",
    "        else:\n",
    "            output = \"\"\n",
    "            if sort_json_key:\n",
    "                keys = sorted(obj.keys(), reverse=True)\n",
    "            else:\n",
    "                keys = obj.keys()\n",
    "            for k in keys:\n",
    "                if update_special_tokens_for_json_key:\n",
    "                    new_special_tokens.append(fr\"<s_{k}>\") if fr\"<s_{k}>\" not in new_special_tokens else None\n",
    "                    new_special_tokens.append(fr\"</s_{k}>\") if fr\"</s_{k}>\" not in new_special_tokens else None\n",
    "                output += (\n",
    "                    fr\"<s_{k}>\"\n",
    "                    + json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                    + fr\"</s_{k}>\"\n",
    "                )\n",
    "            return output\n",
    "    elif type(obj) == list:\n",
    "        return r\"<sep/>\".join(\n",
    "            [json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "        )\n",
    "    else:\n",
    "        obj = str(obj)\n",
    "        if f\"<{obj}/>\" in new_special_tokens:\n",
    "            obj = f\"<{obj}/>\"\n",
    "        return obj\n",
    "\n",
    "\n",
    "def preprocess_documents_for_donut(sample):\n",
    "    text = json.loads(sample[\"text\"])\n",
    "    d_doc = task_token_start + json2token(text) + task_token_end\n",
    "\n",
    "    image = sample[\"image\"].convert('RGB')  # our image is monochrome but models wants RGB\n",
    "    return {\"image\": image, \"text\": d_doc}\n",
    "\n",
    "\n",
    "proc_dataset = dataset.map(preprocess_documents_for_donut)\n",
    "\n",
    "print(f\"Sample: {proc_dataset['train'][45]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b6d11-dcfc-4601-aff3-ecdc978ae65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1216a72b-12ee-4183-825c-f7a72c24ffb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "210a80da-bdc8-4c73-a41c-7c783ef40f0a",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699534cc-80bc-4040-906b-602b5e02f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "processor.tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": new_special_tokens + [task_token_start] + [task_token_end]},\n",
    ")\n",
    "\n",
    "processor.feature_extractor.size = list(dataset['train'][0]['image'].size)\n",
    "processor.feature_extractor.do_align_long_axis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35444353-cbc6-4c5b-abac-265d776168bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_tokenize(\n",
    "    sample, \n",
    "    processor=processor, \n",
    "    split=\"train\", \n",
    "    max_length=512, \n",
    "    ignore_id=-100,\n",
    "):\n",
    "    pixel_values = processor(\n",
    "        sample[\"image\"], random_padding=split == \"train\", return_tensors=\"pt\"\n",
    "    ).pixel_values.squeeze()\n",
    "\n",
    "\n",
    "    input_ids = processor.tokenizer(\n",
    "        sample[\"text\"],\n",
    "        add_special_tokens=False,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = ignore_id\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values, \n",
    "        \"labels\": labels, \n",
    "        \"target_sequence\": sample[\"text\"],\n",
    "    }\n",
    "\n",
    "\n",
    "processed_dataset = proc_dataset.map(\n",
    "    transform_and_tokenize,\n",
    "    remove_columns=[\"image\",\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474c243-bcf5-485f-b683-2065e373ce06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca3e35-6d77-4a3b-8666-6d809b13a3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a9b18d-9196-427d-a058-3f6efa65f863",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64164e34-0521-46c0-bbd8-3319151fb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = processed_dataset['train'].train_test_split(test_size=0.15)\n",
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac5962-36dc-4177-b356-978ef2eaf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "new_emb = model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "model.config.encoder.image_size = processor.feature_extractor.size[::-1]\n",
    "model.config.decoder.max_length = len(\n",
    "    max(processed_dataset[\"train\"][\"labels\"], key=len),\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s>'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fa65c-9503-4ceb-b003-aac82f5a7f6e",
   "metadata": {},
   "source": [
    "I'll train this on the CPU as my GPU has insufficient memory. The dataset is small enough such that even on CPU it is still pretty quick (~38min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe19e1-e275-40bd-981c-71c026177425",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='output',\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=40,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    no_cuda=True,\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=\"donut-base-mysterybox\",\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d65741-7393-405c-bfc6-3353e558f4b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0c1b3-1c40-45c6-9165-81fd4ca39dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2504d-49f6-4329-bb37-02c99fd5ba86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9053068-a72f-40d5-869c-361a8dfd5030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f775da6-b4a6-4329-9368-080505ffb6fa",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceac89c-fc2d-4bcc-9cb5-65c607ff707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(sample, model=model, processor=processor):\n",
    "    # prepare inputs\n",
    "    pixel_values = torch.tensor(sample[\"pixel_values\"]).unsqueeze(0)\n",
    "    task_prompt = \"<s>\"\n",
    "    decoder_input_ids = processor.tokenizer(\n",
    "        task_prompt, \n",
    "        add_special_tokens=False, \n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "\n",
    "    # run inference\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    # process output\n",
    "    prediction = processor.batch_decode(outputs.sequences)[0]\n",
    "    prediction = processor.token2json(prediction)\n",
    "\n",
    "    # load reference target\n",
    "    target = processor.token2json(sample[\"target_sequence\"])\n",
    "    return prediction, target\n",
    "\n",
    "same = 0\n",
    "for example in processed_dataset[\"test\"]:\n",
    "    prediction, target = run_prediction(example)\n",
    "    print(f\"Reference: {target}   --> Target: {prediction}\")\n",
    "    if prediction == target:\n",
    "        same += 1\n",
    "\n",
    "print(f\"Total of {same}/{len(processed_dataset['test'])} correct\")\n",
    "print(f\"Accuracy: {same / len(processed_dataset['test']):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485c39b-4129-4214-995a-0176c35ac959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97ab5a-5bae-40a2-9f16-b03d3f3af23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f78a587-4c65-48a7-9bae-219370f2065f",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c200a-79fc-424d-92f0-ca9ebaced9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = Path(\"../data/donut-base-mysterybox\")\n",
    "\n",
    "# if not model_path.exists():\n",
    "#     model_path.mkdir()\n",
    "    \n",
    "# trainer.save_model(model_path / \"model\")\n",
    "# processor.save_pretrained(model_path / \"processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aed9d7-9b6c-4f52-b0f1-94219d96fa34",
   "metadata": {},
   "source": [
    "#### push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc212f-456a-4b37-a134-defc584e147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()\n",
    "processor.push_to_hub(\"bakhuisdennis/donut-base-mysterybox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ebc72-3cbd-472c-b999-88847b4a3691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba2f5c-1d3f-47fe-98b0-70207860962c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
